{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(file_path):\n",
    "    word_set = set()\n",
    "    pos_set = set()\n",
    "    chunk_set = set()\n",
    "    tag_set = set()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                word, pos, chunk, tag = line.split()\n",
    "                word_set.add(word)\n",
    "                pos_set.add(pos)\n",
    "                chunk_set.add(chunk)\n",
    "                tag_set.add(tag)\n",
    "\n",
    "    word2idx = {word: idx + 2 for idx, word in enumerate(sorted(word_set))}\n",
    "    word2idx[\"<PAD>\"] = 0\n",
    "    word2idx[\"<UNK>\"] = 1\n",
    "\n",
    "    pos2idx = {pos: idx for idx, pos in enumerate(sorted(pos_set))}\n",
    "    chunk2idx = {chunk: idx for idx, chunk in enumerate(sorted(chunk_set))}\n",
    "    tag2idx = {tag: idx + 1 for idx, tag in enumerate(sorted(tag_set))}\n",
    "    tag2idx[\"O\"] = 0 \n",
    "\n",
    "    return word2idx, pos2idx, chunk2idx, tag2idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelNERDataset(Dataset):\n",
    "    def __init__(self, file_path, word2idx, pos2idx, chunk2idx, tag2idx, max_len):\n",
    "        self.data = self._read_file(file_path)\n",
    "        self.word2idx = word2idx\n",
    "        self.pos2idx = pos2idx\n",
    "        self.chunk2idx = chunk2idx\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "        self.pad_idx = word2idx[\"<PAD>\"]\n",
    "\n",
    "    def _read_file(self, file_path):\n",
    "        \"\"\"Read the data file and return a list of sentences with words, POS, chunk, and tags.\"\"\"\n",
    "        sentences = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            words, pos_tags, chunks, tags = [], [], [], []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    word, pos, chunk, tag = line.split()\n",
    "                    words.append(word)\n",
    "                    pos_tags.append(pos)\n",
    "                    chunks.append(chunk)\n",
    "                    tags.append(tag)\n",
    "                else:\n",
    "                    if words:\n",
    "                        sentences.append((words, pos_tags, chunks, tags))\n",
    "                        words, pos_tags, chunks, tags = [], [], [], []\n",
    "            # Append the last sentence if file doesn't end with a newline\n",
    "            if words:\n",
    "                sentences.append((words, pos_tags, chunks, tags))\n",
    "        return sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words, pos_tags, chunks, tags = self.data[idx]\n",
    "\n",
    "        # Convert words, pos, chunk, and tags to indices\n",
    "        word_indices = [self.word2idx.get(w, self.word2idx[\"<UNK>\"]) for w in words]\n",
    "        pos_indices = [self.pos2idx.get(p, 0) for p in pos_tags]\n",
    "        chunk_indices = [self.chunk2idx.get(c, 0) for c in chunks]\n",
    "        tag_indices = [self.tag2idx.get(t, 0) for t in tags]\n",
    "\n",
    "        # Padding/truncation\n",
    "        word_indices = word_indices[:self.max_len] + [self.pad_idx] * (self.max_len - len(word_indices))\n",
    "        pos_indices = pos_indices[:self.max_len] + [0] * (self.max_len - len(pos_indices))\n",
    "        chunk_indices = chunk_indices[:self.max_len] + [0] * (self.max_len - len(chunk_indices))\n",
    "        tag_indices = tag_indices[:self.max_len] + [0] * (self.max_len - len(tag_indices))\n",
    "\n",
    "        return (\n",
    "            torch.tensor(word_indices, dtype=torch.long),\n",
    "            torch.tensor(pos_indices, dtype=torch.long),\n",
    "            torch.tensor(chunk_indices, dtype=torch.long),\n",
    "            torch.tensor(tag_indices, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    words, pos_tags, chunks, tags = zip(*batch)\n",
    "    return (\n",
    "        torch.stack(words, dim=0),\n",
    "        torch.stack(pos_tags, dim=0),\n",
    "        torch.stack(chunks, dim=0),\n",
    "        torch.stack(tags, dim=0),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMNERMultiLabel(nn.Module):\n",
    "    def __init__(self, vocab_size, pos_size, chunk_size, ner_size, embed_dim, hidden_dim):\n",
    "        super(BiLSTMNERMultiLabel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "\n",
    "        # Linear layers for multi-label outputs\n",
    "        self.fc_pos = nn.Linear(hidden_dim * 2, pos_size)  # POS head\n",
    "        self.fc_chunk = nn.Linear(hidden_dim * 2, chunk_size)  # Chunk head\n",
    "        self.fc_ner = nn.Linear(hidden_dim * 2, ner_size)  # NER head\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Shape: (batch_size, max_len, embed_dim)\n",
    "        lstm_out, _ = self.lstm(x)  # Shape: (batch_size, max_len, hidden_dim * 2)\n",
    "\n",
    "        # Multi-label outputs\n",
    "        pos_out = self.fc_pos(lstm_out)  # Shape: (batch_size, max_len, pos_size)\n",
    "        chunk_out = self.fc_chunk(lstm_out)  # Shape: (batch_size, max_len, chunk_size)\n",
    "        ner_out = self.fc_ner(lstm_out)  # Shape: (batch_size, max_len, ner_size)\n",
    "\n",
    "        return pos_out, chunk_out, ner_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(pos_out, chunk_out, ner_out, pos_labels, chunk_labels, ner_labels, pos_weight=0.5, chunk_weight=0.5, ner_weight=1.0):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    pos_loss = criterion(pos_out.view(-1, pos_out.shape[-1]), pos_labels.view(-1))\n",
    "    chunk_loss = criterion(chunk_out.view(-1, chunk_out.shape[-1]), chunk_labels.view(-1))\n",
    "    ner_loss = criterion(ner_out.view(-1, ner_out.shape[-1]), ner_labels.view(-1))\n",
    "    total_loss = pos_weight * pos_loss + chunk_weight * chunk_loss + ner_weight * ner_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, device, pos_weight=1.0, chunk_weight=1.0, ner_weight=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training\", unit=\"batch\"):\n",
    "        words, pos_labels, chunk_labels, ner_labels = batch\n",
    "        words = words.to(device)\n",
    "        pos_labels = pos_labels.to(device)\n",
    "        chunk_labels = chunk_labels.to(device)\n",
    "        ner_labels = ner_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pos_out, chunk_out, ner_out = model(words)\n",
    "        loss = compute_loss(pos_out, chunk_out, ner_out, pos_labels, chunk_labels, ner_labels, pos_weight, chunk_weight, ner_weight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, idx2pos, idx2chunk, idx2tag, device):\n",
    "    model.eval()\n",
    "    all_preds_pos, all_labels_pos = [], []\n",
    "    all_preds_chunk, all_labels_chunk = [], []\n",
    "    all_preds_ner, all_labels_ner = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            words, pos_tags, chunk_tags, ner_tags = batch\n",
    "            words = words.to(device)\n",
    "            pos_tags = pos_tags.to(device)\n",
    "            chunk_tags = chunk_tags.to(device)\n",
    "            ner_tags = ner_tags.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(words)\n",
    "            pos_output, chunk_output, ner_output = outputs[0], outputs[1], outputs[2]\n",
    "            \n",
    "            # Convert predictions to labels\n",
    "            pos_preds = torch.argmax(pos_output, dim=-1)\n",
    "            chunk_preds = torch.argmax(chunk_output, dim=-1)\n",
    "            ner_preds = torch.argmax(ner_output, dim=-1)\n",
    "            \n",
    "            all_preds_pos.extend(pos_preds.cpu().numpy().tolist())\n",
    "            all_labels_pos.extend(pos_tags.cpu().numpy().tolist())\n",
    "\n",
    "            all_preds_chunk.extend(chunk_preds.cpu().numpy().tolist())\n",
    "            all_labels_chunk.extend(chunk_tags.cpu().numpy().tolist())\n",
    "\n",
    "            all_preds_ner.extend(ner_preds.cpu().numpy().tolist())\n",
    "            all_labels_ner.extend(ner_tags.cpu().numpy().tolist())\n",
    "    \n",
    "    # Flatten the lists\n",
    "    all_preds_pos = flatten(all_preds_pos)\n",
    "    all_labels_pos = flatten(all_labels_pos)\n",
    "\n",
    "    all_preds_chunk = flatten(all_preds_chunk)\n",
    "    all_labels_chunk = flatten(all_labels_chunk)\n",
    "\n",
    "    all_preds_ner = flatten(all_preds_ner)\n",
    "    all_labels_ner = flatten(all_labels_ner)\n",
    "\n",
    "    # Exclude 'O' tag for NER\n",
    "    ner_labels_filtered, ner_preds_filtered = [], []\n",
    "    for label, pred in zip(all_labels_ner, all_preds_ner):\n",
    "        if idx2tag[label] != \"O\":\n",
    "            ner_labels_filtered.append(label)\n",
    "            ner_preds_filtered.append(pred)\n",
    "\n",
    "    # Generate classification reports\n",
    "    pos_labels = list(idx2pos.keys())\n",
    "    chunk_labels = list(idx2chunk.keys())\n",
    "    ner_labels = [k for k, v in idx2tag.items() if v != \"O\"]\n",
    "\n",
    "    pos_report = classification_report(all_labels_pos, all_preds_pos, labels=pos_labels, target_names=[idx2pos[i] for i in pos_labels])\n",
    "    chunk_report = classification_report(all_labels_chunk, all_preds_chunk, labels=chunk_labels, target_names=[idx2chunk[i] for i in chunk_labels])\n",
    "    ner_report = classification_report(ner_labels_filtered, ner_preds_filtered, labels=ner_labels, target_names=[idx2tag[i] for i in ner_labels])\n",
    "\n",
    "    return pos_report, chunk_report, ner_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path, vocab):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "    # Save model weights\n",
    "    torch.save(model.state_dict(), f\"{path}.pth\")\n",
    "    \n",
    "    # Save vocab\n",
    "    with open(f\"{path}_vocab.pkl\", \"wb\") as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    \n",
    "    print(f\"Model and vocab saved to {path}.pth and {path}_vocab.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_class, path, device, embed_dim, hidden_dim):\n",
    "    # Load vocab\n",
    "    with open(f\"{path}_vocab.pkl\", \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    \n",
    "    word2idx, pos2idx, chunk2idx, tag2idx = (\n",
    "        vocab[\"word2idx\"],\n",
    "        vocab[\"pos2idx\"],\n",
    "        vocab[\"chunk2idx\"],\n",
    "        vocab[\"tag2idx\"],\n",
    "    )\n",
    "    \n",
    "    # Initialize and load model\n",
    "    model = model_class(\n",
    "        len(word2idx),\n",
    "        len(pos2idx),\n",
    "        len(chunk2idx),\n",
    "        len(tag2idx),\n",
    "        embed_dim,\n",
    "        hidden_dim,\n",
    "    )\n",
    "    model.load_state_dict(torch.load(f\"{path}.pth\", map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Model and vocab loaded successfully.\")\n",
    "    return model, word2idx, pos2idx, chunk2idx, tag2idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text, word2idx, pos2idx, chunk2idx, idx2tag, max_len, device):\n",
    "    words = text.split()\n",
    "    word_indices = [word2idx.get(w, word2idx[\"<UNK>\"]) for w in words]\n",
    "    word_indices = word_indices[:max_len] + [word2idx[\"<PAD>\"]] * (max_len - len(word_indices))\n",
    "    \n",
    "    model_input = torch.tensor([word_indices]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, _, ner_out = model(model_input) \n",
    "        predictions = torch.sigmoid(ner_out).cpu().numpy()\n",
    "    \n",
    "    predicted_tags = []\n",
    "    for word_pred in predictions[0][:len(words)]:\n",
    "        max_prob_idx = word_pred.argmax() \n",
    "        predicted_tags.append([idx2tag[max_prob_idx]])\n",
    "    \n",
    "    result = [(w, tags if tags else [\"O\"]) for w, tags in zip(words, predicted_tags)]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "train_file = \"data/eng/eng.train\"\n",
    "val_file = \"data/eng/eng.testa\"\n",
    "test_file = \"data/eng/eng.testb\"\n",
    "\n",
    "# Build vocabulary\n",
    "word2idx, pos2idx, chunk2idx, tag2idx = create_vocab(train_file)\n",
    "idx2pos = {idx: pos for pos, idx in pos2idx.items()}\n",
    "idx2chunk = {idx: chunk for chunk, idx in chunk2idx.items()}\n",
    "idx2tag = {idx: tag for tag, idx in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 1,\n",
       " 'B-MISC': 2,\n",
       " 'B-ORG': 3,\n",
       " 'I-LOC': 4,\n",
       " 'I-MISC': 5,\n",
       " 'I-ORG': 6,\n",
       " 'I-PER': 7,\n",
       " 'O': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "hidden_dim = 256\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = MultiLabelNERDataset(train_file, word2idx, pos2idx, chunk2idx, tag2idx, max_len=50)\n",
    "val_dataset = MultiLabelNERDataset(val_file, word2idx, pos2idx, chunk2idx, tag2idx, max_len=50)\n",
    "test_dataset = MultiLabelNERDataset(test_file, word2idx, pos2idx, chunk2idx, tag2idx, max_len=50)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_loader.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMNERMultiLabel(len(word2idx), len(pos2idx), len(chunk2idx), len(tag2idx), embed_dim, hidden_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 79.24batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.8785\n",
      "Validation:\n",
      "NER Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         0\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       B-ORG       0.00      0.00      0.00         0\n",
      "       I-LOC       0.86      0.36      0.51      2088\n",
      "      I-MISC       0.85      0.11      0.19      1258\n",
      "       I-ORG       0.78      0.25      0.38      2085\n",
      "       I-PER       0.89      0.37      0.52      3053\n",
      "\n",
      "   micro avg       0.85      0.30      0.44      8488\n",
      "   macro avg       0.48      0.16      0.23      8488\n",
      "weighted avg       0.85      0.30      0.44      8488\n",
      "\n",
      "Epoch [2/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 84.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3334\n",
      "Validation:\n",
      "NER Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         0\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       B-ORG       0.00      0.00      0.00         0\n",
      "       I-LOC       0.89      0.60      0.72      2088\n",
      "      I-MISC       0.84      0.50      0.62      1258\n",
      "       I-ORG       0.78      0.52      0.62      2085\n",
      "       I-PER       0.82      0.77      0.79      3053\n",
      "\n",
      "   micro avg       0.83      0.63      0.71      8488\n",
      "   macro avg       0.48      0.34      0.39      8488\n",
      "weighted avg       0.83      0.63      0.71      8488\n",
      "\n",
      "Epoch [3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 83.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2205\n",
      "Validation:\n",
      "NER Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         0\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       B-ORG       0.00      0.00      0.00         0\n",
      "       I-LOC       0.92      0.69      0.79      2088\n",
      "      I-MISC       0.88      0.62      0.72      1258\n",
      "       I-ORG       0.82      0.61      0.70      2085\n",
      "       I-PER       0.84      0.85      0.84      3053\n",
      "\n",
      "   micro avg       0.86      0.72      0.78      8488\n",
      "   macro avg       0.50      0.39      0.44      8488\n",
      "weighted avg       0.86      0.72      0.78      8488\n",
      "\n",
      "Epoch [4/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 83.49batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1550\n",
      "Validation:\n",
      "NER Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         0\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       B-ORG       0.00      0.00      0.00         0\n",
      "       I-LOC       0.94      0.73      0.82      2088\n",
      "      I-MISC       0.92      0.65      0.76      1258\n",
      "       I-ORG       0.78      0.72      0.75      2085\n",
      "       I-PER       0.90      0.83      0.86      3053\n",
      "\n",
      "   micro avg       0.88      0.75      0.81      8488\n",
      "   macro avg       0.51      0.42      0.46      8488\n",
      "weighted avg       0.88      0.75      0.81      8488\n",
      "\n",
      "Epoch [5/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 82.86batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1102\n",
      "Validation:\n",
      "NER Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         0\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       B-ORG       0.00      0.00      0.00         0\n",
      "       I-LOC       0.91      0.79      0.85      2088\n",
      "      I-MISC       0.91      0.71      0.80      1258\n",
      "       I-ORG       0.89      0.69      0.77      2085\n",
      "       I-PER       0.87      0.89      0.88      3053\n",
      "\n",
      "   micro avg       0.89      0.79      0.84      8488\n",
      "   macro avg       0.51      0.44      0.47      8488\n",
      "weighted avg       0.89      0.79      0.83      8488\n",
      "\n",
      "Epoch [6/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 82.85batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0775\n",
      "Validation:\n",
      "NER Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         0\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       B-ORG       0.00      0.00      0.00         0\n",
      "       I-LOC       0.89      0.85      0.87      2088\n",
      "      I-MISC       0.91      0.74      0.82      1258\n",
      "       I-ORG       0.87      0.73      0.79      2085\n",
      "       I-PER       0.92      0.86      0.89      3053\n",
      "\n",
      "   micro avg       0.90      0.81      0.85      8488\n",
      "   macro avg       0.51      0.45      0.48      8488\n",
      "weighted avg       0.90      0.81      0.85      8488\n",
      "\n",
      "Epoch [7/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 82.45batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0531\n",
      "Validation:\n",
      "NER Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         0\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       B-ORG       0.00      0.00      0.00         0\n",
      "       I-LOC       0.92      0.84      0.88      2088\n",
      "      I-MISC       0.93      0.73      0.82      1258\n",
      "       I-ORG       0.87      0.73      0.80      2085\n",
      "       I-PER       0.92      0.85      0.89      3053\n",
      "\n",
      "   micro avg       0.91      0.80      0.85      8488\n",
      "   macro avg       0.52      0.45      0.48      8488\n",
      "weighted avg       0.91      0.80      0.85      8488\n",
      "\n",
      "Epoch [8/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 82.54batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0352\n",
      "Validation:\n",
      "NER Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         0\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       B-ORG       0.00      0.00      0.00         0\n",
      "       I-LOC       0.93      0.81      0.87      2088\n",
      "      I-MISC       0.91      0.74      0.82      1258\n",
      "       I-ORG       0.88      0.74      0.80      2085\n",
      "       I-PER       0.92      0.87      0.89      3053\n",
      "\n",
      "   micro avg       0.91      0.80      0.85      8488\n",
      "   macro avg       0.52      0.45      0.48      8488\n",
      "weighted avg       0.91      0.80      0.85      8488\n",
      "\n",
      "Epoch [9/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 81.79batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0223\n",
      "Validation:\n",
      "NER Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         0\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       B-ORG       0.00      0.00      0.00         0\n",
      "       I-LOC       0.93      0.83      0.87      2088\n",
      "      I-MISC       0.89      0.76      0.82      1258\n",
      "       I-ORG       0.89      0.74      0.81      2085\n",
      "       I-PER       0.93      0.84      0.89      3053\n",
      "\n",
      "   micro avg       0.92      0.80      0.85      8488\n",
      "   macro avg       0.52      0.45      0.48      8488\n",
      "weighted avg       0.91      0.80      0.85      8488\n",
      "\n",
      "Epoch [10/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 81.72batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0137\n",
      "Validation:\n",
      "NER Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         0\n",
      "      B-MISC       0.20      0.25      0.22         4\n",
      "       B-ORG       0.00      0.00      0.00         0\n",
      "       I-LOC       0.92      0.84      0.88      2088\n",
      "      I-MISC       0.91      0.75      0.82      1258\n",
      "       I-ORG       0.88      0.74      0.80      2085\n",
      "       I-PER       0.93      0.84      0.88      3053\n",
      "\n",
      "   micro avg       0.91      0.80      0.85      8488\n",
      "   macro avg       0.55      0.49      0.52      8488\n",
      "weighted avg       0.91      0.80      0.85      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "    train_loss = train_model(model, train_loader, optimizer, device)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    print(\"Validation:\")\n",
    "    pos_report, chunk_report, ner_report = evaluate_model(model, val_loader, idx2pos, idx2chunk, idx2tag, device)\n",
    "    print(\"NER Classification Report:\\n\", ner_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vocab saved to save/models/multilabel_bilstm_ok.pth and save/models/multilabel_bilstm_ok_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "# save_model(model, \"save/models/multilabel_bilstm_1\", {\n",
    "#     \"word2idx\": word2idx,\n",
    "#     \"pos2idx\": pos2idx,\n",
    "#     \"chunk2idx\": chunk2idx,\n",
    "#     \"tag2idx\": tag2idx,\n",
    "# })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vocab loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# loaded_model, loaded_word2idx, loaded_pos2idx, loaded_chunk2idx, loaded_tag2idx = load_model(\n",
    "#     BiLSTMNERMultiLabel,\n",
    "#     \"save/models/multilabel_bilstm\",\n",
    "#     device,\n",
    "#     embed_dim,\n",
    "#     hidden_dim\n",
    "# )\n",
    "# loaded_idx2tag = {idx: tag for tag, idx in loaded_tag2idx.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_test_exclude_o(model, test_loader, idx2tag, device):\n",
    "    model.eval()\n",
    "    all_preds_ner, all_labels_ner = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            words, pos_tags, chunk_tags, ner_tags = batch\n",
    "            words = words.to(device)\n",
    "            pos_tags = pos_tags.to(device)\n",
    "            chunk_tags = chunk_tags.to(device)\n",
    "            ner_tags = ner_tags.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(words)\n",
    "            _, _, ner_output = outputs  # Outputs: (POS, Chunk, NER)\n",
    "            \n",
    "            # Convert predictions to labels\n",
    "            ner_preds = torch.argmax(ner_output, dim=-1)  # Shape: [batch_size, seq_len]\n",
    "\n",
    "            # Extend lists with predictions and labels (remove padding tokens)\n",
    "            for pred, label in zip(ner_preds.cpu().numpy(), ner_tags.cpu().numpy()):\n",
    "                for p, l in zip(pred, label):\n",
    "                    if idx2tag[l] != \"O\":  # Exclude 'O' tags\n",
    "                        all_preds_ner.append(p)\n",
    "                        all_labels_ner.append(l)\n",
    "    \n",
    "    # Convert to tags\n",
    "    predicted_tags = [idx2tag[idx] for idx in all_preds_ner]\n",
    "    true_tags = [idx2tag[idx] for idx in all_labels_ner]\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_tags, predicted_tags)\n",
    "\n",
    "    return accuracy, predicted_tags, true_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, predicted_tags, true_tags = evaluate_on_test_exclude_o(model, test_loader, idx2tag, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7390054972513743"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I-ORG', 'I-LOC'),\n",
       " ('O', 'I-PER'),\n",
       " ('I-ORG', 'I-PER'),\n",
       " ('O', 'I-PER'),\n",
       " ('I-LOC', 'I-LOC'),\n",
       " ('I-LOC', 'I-LOC'),\n",
       " ('I-LOC', 'I-LOC'),\n",
       " ('I-ORG', 'I-LOC'),\n",
       " ('I-LOC', 'I-LOC'),\n",
       " ('I-MISC', 'I-MISC')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(predicted_tags, true_tags))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 82.35batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0082\n",
      "Epoch [2/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 82.20batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0051\n",
      "Epoch [3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 82.38batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0031\n",
      "Epoch [4/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 82.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0022\n",
      "Epoch [5/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 83.65batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0019\n",
      "Epoch [6/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 82.53batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0016\n",
      "Epoch [7/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 82.37batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0025\n",
      "Epoch [8/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 82.57batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0064\n",
      "Epoch [9/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 82.47batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0033\n",
      "Epoch [10/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 80.88batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "    train_loss = train_model(model, train_loader, optimizer, device)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('EU', ['I-ORG']), ('rejects', ['O']), ('German', ['I-MISC']), ('call', ['O']), ('to', ['O']), ('boycott', ['O']), ('British', ['I-MISC']), ('lamb', ['O'])]\n"
     ]
    }
   ],
   "source": [
    "# text = \"The European Union is headquartered in Brussels\"\n",
    "# text = \"EU rejects German call to boycott British lamb\"\n",
    "# predictions = predict(\n",
    "#     loaded_model,\n",
    "#     text,\n",
    "#     loaded_word2idx,\n",
    "#     loaded_pos2idx,\n",
    "#     loaded_chunk2idx,\n",
    "#     loaded_idx2tag,\n",
    "#     max_len=50,\n",
    "#     device=device\n",
    "# )\n",
    "# print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, predicted_tags, true_tags = evaluate_on_test_exclude_o(model, test_loader, idx2tag, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7433783108445777"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I-LOC', 'I-LOC'),\n",
       " ('I-ORG', 'I-MISC'),\n",
       " ('I-LOC', 'I-LOC'),\n",
       " ('I-ORG', 'I-ORG'),\n",
       " ('I-ORG', 'I-ORG'),\n",
       " ('I-LOC', 'I-LOC'),\n",
       " ('O', 'I-MISC'),\n",
       " ('I-MISC', 'I-MISC'),\n",
       " ('I-MISC', 'I-MISC'),\n",
       " ('I-PER', 'I-PER')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(predicted_tags, true_tags))[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-LOC       0.90      0.77      0.83      1905\n",
      "      I-MISC       0.87      0.67      0.75       908\n",
      "       I-ORG       0.84      0.72      0.77      2480\n",
      "       I-PER       0.91      0.78      0.84      2691\n",
      "\n",
      "   micro avg       0.88      0.74      0.81      8004\n",
      "   macro avg       0.50      0.42      0.46      8004\n",
      "weighted avg       0.88      0.74      0.80      8004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos_report, chunk_report, ner_report = evaluate_model(model, test_loader, idx2pos, idx2chunk, idx2tag, device)\n",
    "print(\"NER Classification Report:\\n\", ner_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
