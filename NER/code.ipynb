{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(file_path):\n",
    "    word2idx = defaultdict(lambda: len(word2idx))\n",
    "    tag2idx = defaultdict(lambda: len(tag2idx))\n",
    "    word2idx[\"<PAD>\"] = 0\n",
    "    # tag2idx[\"O\"] = 0\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                parts = line.strip().split(' ')\n",
    "                word, tag = parts[0], parts[3]\n",
    "                word2idx[word]\n",
    "                tag2idx[tag]\n",
    "    return dict(word2idx), dict(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, file_path, word2idx, tag2idx, max_len):\n",
    "        self.sentences, self.labels = self._read_data(file_path)\n",
    "        self.word2idx = word2idx\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def _read_data(self, file_path):\n",
    "        sentences, labels = [], []\n",
    "        sentence, label = [], []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip() == \"\":\n",
    "                    if sentence:\n",
    "                        sentences.append(sentence)\n",
    "                        labels.append(label)\n",
    "                        sentence, label = [], []\n",
    "                else:\n",
    "                    parts = line.strip().split()\n",
    "                    word, tag = parts[0], parts[3]\n",
    "                    sentence.append(word)\n",
    "                    label.append(tag)\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "        return sentences, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert to indices\n",
    "        word_indices = [self.word2idx.get(w, self.word2idx[\"<PAD>\"]) for w in sentence]\n",
    "        tag_indices = [self.tag2idx.get(t, self.tag2idx[\"O\"]) for t in label]\n",
    "\n",
    "        # Pad sequences\n",
    "        word_indices = word_indices[:self.max_len] + [self.word2idx[\"<PAD>\"]] * (self.max_len - len(word_indices))\n",
    "        tag_indices = tag_indices[:self.max_len] + [self.tag2idx[\"O\"]] * (self.max_len - len(tag_indices))\n",
    "\n",
    "        return torch.tensor(word_indices), torch.tensor(tag_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Pretrained Embeddings and Prepare Dictionaries\n",
    "def load_pretrained_embeddings(file_path, word2idx):\n",
    "    embedding_matrix = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            if word in word2idx:\n",
    "                embedding_matrix.append(vector)\n",
    "    return np.array(embedding_matrix)\n",
    "\n",
    "\n",
    "def load_pretrained_embeddings(file_path, embedding_type='glove'):\n",
    "    embeddings = {}\n",
    "\n",
    "    if embedding_type == 'glove':  # GloVe Format\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "\n",
    "    elif embedding_type == 'word2vec':  # Word2Vec Text Format\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            next(f)  # Skip the first line (header line with vocab size, dimensions)\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "\n",
    "    elif embedding_type == 'fasttext':  # FastText Format\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            next(f)  # Skip the first line (header line)\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported embedding type: {embedding_type}\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def create_embedding_matrix(vocab, pretrained_embeddings, embedding_dim, unk_token='<UNK>'):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, idx in vocab.items():\n",
    "        if word in pretrained_embeddings:\n",
    "            embedding_matrix[idx] = pretrained_embeddings[word]\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.normal(size=(embedding_dim,))  # Random for unknowns\n",
    "\n",
    "    # Handle the unknown token if present\n",
    "    if unk_token in vocab:\n",
    "        embedding_matrix[vocab[unk_token]] = np.random.normal(size=(embedding_dim,))\n",
    "\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMNER(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_size, embed_dim, hidden_dim):\n",
    "        super(BiLSTMNER, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, tag_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for words, tags in tqdm(loader, desc=\"Training\", unit=\"batch\"):\n",
    "        words, tags = words.to(device), tags.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(words)\n",
    "        # Flatten for loss computation\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])  \n",
    "        tags = tags.view(-1)\n",
    "        loss = criterion(outputs, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, idx2tag, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for words, tags in loader:\n",
    "            words, tags = words.to(device), tags.to(device)\n",
    "            outputs = model(words)\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            all_preds.extend(predictions.view(-1).tolist())\n",
    "            all_labels.extend(tags.view(-1).tolist())\n",
    "    # Remove padding tokens\n",
    "    valid_preds = [idx2tag[p] for p, l in zip(all_preds, all_labels) if idx2tag[l] != \"O\"]\n",
    "    valid_labels = [idx2tag[l] for p, l in zip(all_preds, all_labels) if idx2tag[l] != \"O\"]\n",
    "    # valid_preds = [idx2tag[p] for p, l in zip(all_preds, all_labels)]\n",
    "    # valid_labels = [idx2tag[l] for p, l in zip(all_preds, all_labels)]\n",
    "    return classification_report(valid_labels, valid_preds, output_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path, word2idx, tag2idx):\n",
    "    torch.save(model.state_dict(), f\"{path}.pth\")\n",
    "    with open(f\"{path}_vocab.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"word2idx\": word2idx, \"tag2idx\": tag2idx}, f)\n",
    "    print(f\"Model and vocab saved to {path}.pt and {path}_vocab.pkl\")\n",
    "\n",
    "def load_model(model_class, path, vocab_path, embed_dim, hidden_dim, device):\n",
    "    with open(vocab_path, \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    word2idx, tag2idx = vocab[\"word2idx\"], vocab[\"tag2idx\"]\n",
    "    model = model_class(len(word2idx), len(tag2idx), embed_dim, hidden_dim)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model, word2idx, tag2idx\n",
    "\n",
    "def predict(model, text, word2idx, idx2tag, max_len, device):\n",
    "    words = text.split()\n",
    "    word_indices = [word2idx.get(w, word2idx[\"<PAD>\"]) for w in words]\n",
    "    word_indices = word_indices[:max_len] + [word2idx[\"<PAD>\"]] * (max_len - len(word_indices))\n",
    "    \n",
    "    model_input = torch.tensor([word_indices]).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(model_input)\n",
    "        predictions = torch.argmax(outputs, dim=-1).squeeze(0).tolist()\n",
    "\n",
    "    # Convert predictions to tags\n",
    "    tags = [idx2tag[idx] for idx in predictions[:len(words)]]\n",
    "    return list(zip(words, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "train_file = \"data/eng/eng.train\"\n",
    "val_file = \"data/eng/eng.testa\"\n",
    "test_file = \"data/eng/eng.testb\"\n",
    "\n",
    "# Build vocabulary\n",
    "word2idx, tag2idx = build_vocab(train_file)\n",
    "idx2tag = {idx: tag for tag, idx in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_dim = 100\n",
    "hidden_dim = 128\n",
    "max_len = 50\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(train_file, word2idx, tag2idx, max_len)\n",
    "val_dataset = NERDataset(val_file, word2idx, tag2idx, max_len)\n",
    "test_dataset = NERDataset(test_file, word2idx, tag2idx, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate class weights\n",
    "# num_tags = len(tag2idx)\n",
    "# tag_counts = [0] * num_tags\n",
    "# for _, labels in train_dataset:\n",
    "#     for tag in labels.tolist():\n",
    "#         tag_counts[tag] += 1\n",
    "# total_tags = sum(tag_counts)\n",
    "# class_weights = [total_tags / count if count > 0 else 0.0 for count in tag_counts]\n",
    "\n",
    "# # Convert to tensor and move to device\n",
    "# weights = torch.tensor(class_weights).to(device)\n",
    "\n",
    "# # Define loss function with weights\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=0, weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, ignore_index=-1):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(ignore_index=self.ignore_index, reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)  # Probabilities of the true class\n",
    "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Replace criterion with FocalLoss\n",
    "criterion = FocalLoss(alpha=1, gamma=2, ignore_index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, optimizer, loss\n",
    "model = BiLSTMNER(len(word2idx), len(tag2idx), embed_dim, hidden_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:02<00:00, 183.71batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0823\n",
      "Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       I-LOC       0.85      0.50      0.63      2088\n",
      "      I-MISC       0.86      0.31      0.45      1258\n",
      "       I-ORG       0.00      0.00      0.00      2085\n",
      "       I-PER       0.90      0.51      0.65      3053\n",
      "           O       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.35      8488\n",
      "   macro avg       0.43      0.22      0.29      8488\n",
      "weighted avg       0.66      0.35      0.45      8488\n",
      "\n",
      "===================================================================================\n",
      "Epoch [2/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:02<00:00, 202.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0190\n",
      "Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       I-LOC       0.84      0.66      0.74      2088\n",
      "      I-MISC       0.86      0.60      0.71      1258\n",
      "       I-ORG       0.00      0.00      0.00      2085\n",
      "       I-PER       0.92      0.64      0.75      3053\n",
      "           O       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.48      8488\n",
      "   macro avg       0.44      0.32      0.37      8488\n",
      "weighted avg       0.66      0.48      0.56      8488\n",
      "\n",
      "===================================================================================\n",
      "Epoch [3/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:02<00:00, 204.87batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0098\n",
      "Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       I-LOC       0.83      0.74      0.78      2088\n",
      "      I-MISC       0.88      0.66      0.75      1258\n",
      "       I-ORG       0.00      0.00      0.00      2085\n",
      "       I-PER       0.92      0.65      0.76      3053\n",
      "           O       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.51      8488\n",
      "   macro avg       0.44      0.34      0.38      8488\n",
      "weighted avg       0.67      0.51      0.58      8488\n",
      "\n",
      "===================================================================================\n",
      "Epoch [4/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:02<00:00, 203.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0051\n",
      "Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       I-LOC       0.86      0.73      0.79      2088\n",
      "      I-MISC       0.88      0.69      0.77      1258\n",
      "       I-ORG       0.00      0.00      0.00      2085\n",
      "       I-PER       0.91      0.69      0.79      3053\n",
      "           O       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.53      8488\n",
      "   macro avg       0.44      0.35      0.39      8488\n",
      "weighted avg       0.67      0.53      0.59      8488\n",
      "\n",
      "===================================================================================\n",
      "Epoch [5/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:02<00:00, 210.87batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0025\n",
      "Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       I-LOC       0.85      0.77      0.81      2088\n",
      "      I-MISC       0.85      0.73      0.79      1258\n",
      "       I-ORG       0.00      0.00      0.00      2085\n",
      "       I-PER       0.91      0.70      0.79      3053\n",
      "           O       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.55      8488\n",
      "   macro avg       0.44      0.37      0.40      8488\n",
      "weighted avg       0.66      0.55      0.60      8488\n",
      "\n",
      "===================================================================================\n",
      "Final Test Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-LOC       0.81      0.69      0.75      1905\n",
      "      I-MISC       0.77      0.64      0.70       908\n",
      "       I-ORG       0.00      0.00      0.00      2480\n",
      "       I-PER       0.86      0.59      0.70      2691\n",
      "           O       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.44      8004\n",
      "   macro avg       0.30      0.24      0.27      8004\n",
      "weighted avg       0.57      0.44      0.49      8004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and validate\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    print(\"Validation:\")\n",
    "    val_report = evaluate_model(model, val_loader, idx2tag, device)\n",
    "    print(val_report)\n",
    "    print(\"===================================================================================\")\n",
    "\n",
    "# Final test evaluation\n",
    "print(\"Final Test Evaluation:\")\n",
    "test_report = evaluate_model(model, test_loader, idx2tag, device)\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vocab saved to save/models/bilstm.pt and save/models/bilstm_vocab.pkl\n",
      "Model loaded from save/models/bilstm.pt\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model_path = \"save/models/bilstm\"\n",
    "save_model(model, model_path, word2idx, tag2idx)\n",
    "\n",
    "# Load model and predict\n",
    "loaded_model, loaded_word2idx, loaded_tag2idx = load_model(\n",
    "    BiLSTMNER, f\"{model_path}.pt\", f\"{model_path}_vocab.pkl\", embed_dim, hidden_dim, device\n",
    ")\n",
    "loaded_idx2tag = {idx: tag for tag, idx in loaded_tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [('EU', 'I-MISC'), ('rejects', 'I-MISC'), ('German', 'I-MISC'), ('call', 'I-MISC'), ('to', 'O'), ('boycott', 'I-MISC'), ('British', 'I-MISC'), ('lamb', 'I-MISC'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "test_text = \"EU rejects German call to boycott British lamb .\"\n",
    "predictions = predict(\n",
    "    loaded_model, test_text, loaded_word2idx, loaded_idx2tag, max_len, device\n",
    ")\n",
    "print(\"Predictions:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
