{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(file_path):\n",
    "    word_set = set()\n",
    "    pos_set = set()\n",
    "    chunk_set = set()\n",
    "    tag_set = set()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                word, pos, chunk, tag = line.split()\n",
    "                word_set.add(word)\n",
    "                pos_set.add(pos)\n",
    "                chunk_set.add(chunk)\n",
    "                tag_set.add(tag)\n",
    "\n",
    "    word2idx = {word: idx + 2 for idx, word in enumerate(sorted(word_set))}\n",
    "    word2idx[\"<PAD>\"] = 0\n",
    "    word2idx[\"<UNK>\"] = 1\n",
    "\n",
    "    pos2idx = {pos: idx for idx, pos in enumerate(sorted(pos_set))}\n",
    "    chunk2idx = {chunk: idx for idx, chunk in enumerate(sorted(chunk_set))}\n",
    "    tag2idx = {tag: idx + 1 for idx, tag in enumerate(sorted(tag_set))}\n",
    "    tag2idx[\"O\"] = 0 \n",
    "\n",
    "    return word2idx, pos2idx, chunk2idx, tag2idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class MultiLabelNERDataset(Dataset):\n",
    "    def __init__(self, file_path, word2idx, pos2idx, chunk2idx, tag2idx, max_len):\n",
    "        self.data = self._read_file(file_path)\n",
    "        self.word2idx = word2idx\n",
    "        self.pos2idx = pos2idx\n",
    "        self.chunk2idx = chunk2idx\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "        self.pad_idx = word2idx[\"<PAD>\"]\n",
    "\n",
    "    def _read_file(self, file_path):\n",
    "        \"\"\"Read the data file and return a list of sentences with words, POS, chunk, and tags.\"\"\"\n",
    "        sentences = []\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            words, pos_tags, chunks, tags = [], [], [], []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    word, pos, chunk, tag = line.split()\n",
    "                    words.append(word)\n",
    "                    pos_tags.append(pos)\n",
    "                    chunks.append(chunk)\n",
    "                    tags.append(tag)\n",
    "                else:\n",
    "                    if words:\n",
    "                        sentences.append((words, pos_tags, chunks, tags))\n",
    "                        words, pos_tags, chunks, tags = [], [], [], []\n",
    "            # Append the last sentence if file doesn't end with a newline\n",
    "            if words:\n",
    "                sentences.append((words, pos_tags, chunks, tags))\n",
    "        return sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words, pos_tags, chunks, tags = self.data[idx]\n",
    "\n",
    "        # Convert words, pos, chunk, and tags to indices\n",
    "        word_indices = [self.word2idx.get(w, self.word2idx[\"<UNK>\"]) for w in words]\n",
    "        pos_indices = [self.pos2idx.get(p, 0) for p in pos_tags]\n",
    "        chunk_indices = [self.chunk2idx.get(c, 0) for c in chunks]\n",
    "        tag_indices = [self.tag2idx.get(t, 0) for t in tags]\n",
    "\n",
    "        # Padding/truncation\n",
    "        word_indices = word_indices[:self.max_len] + [self.pad_idx] * (self.max_len - len(word_indices))\n",
    "        pos_indices = pos_indices[:self.max_len] + [0] * (self.max_len - len(pos_indices))\n",
    "        chunk_indices = chunk_indices[:self.max_len] + [0] * (self.max_len - len(chunk_indices))\n",
    "        tag_indices = tag_indices[:self.max_len] + [0] * (self.max_len - len(tag_indices))\n",
    "\n",
    "        return (\n",
    "            torch.tensor(word_indices, dtype=torch.long),\n",
    "            torch.tensor(pos_indices, dtype=torch.long),\n",
    "            torch.tensor(chunk_indices, dtype=torch.long),\n",
    "            torch.tensor(tag_indices, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    words, pos_tags, chunks, tags = zip(*batch)\n",
    "    return (\n",
    "        torch.stack(words, dim=0),\n",
    "        torch.stack(pos_tags, dim=0),\n",
    "        torch.stack(chunks, dim=0),\n",
    "        torch.stack(tags, dim=0),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMNERMultiLabel(nn.Module):\n",
    "    def __init__(self, vocab_size, pos_size, chunk_size, ner_size, embed_dim, hidden_dim):\n",
    "        super(BiLSTMNERMultiLabel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "\n",
    "        # Linear layers for multi-label outputs\n",
    "        self.fc_pos = nn.Linear(hidden_dim * 2, pos_size)  # POS head\n",
    "        self.fc_chunk = nn.Linear(hidden_dim * 2, chunk_size)  # Chunk head\n",
    "        self.fc_ner = nn.Linear(hidden_dim * 2, ner_size)  # NER head\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Shape: (batch_size, max_len, embed_dim)\n",
    "        lstm_out, _ = self.lstm(x)  # Shape: (batch_size, max_len, hidden_dim * 2)\n",
    "\n",
    "        # Multi-label outputs\n",
    "        pos_out = self.fc_pos(lstm_out)  # Shape: (batch_size, max_len, pos_size)\n",
    "        chunk_out = self.fc_chunk(lstm_out)  # Shape: (batch_size, max_len, chunk_size)\n",
    "        ner_out = self.fc_ner(lstm_out)  # Shape: (batch_size, max_len, ner_size)\n",
    "\n",
    "        return pos_out, chunk_out, ner_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(pos_out, chunk_out, ner_out, pos_labels, chunk_labels, ner_labels, pos_weight=1.0, chunk_weight=1.0, ner_weight=1.0):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    pos_loss = criterion(pos_out.view(-1, pos_out.shape[-1]), pos_labels.view(-1))\n",
    "    chunk_loss = criterion(chunk_out.view(-1, chunk_out.shape[-1]), chunk_labels.view(-1))\n",
    "    ner_loss = criterion(ner_out.view(-1, ner_out.shape[-1]), ner_labels.view(-1))\n",
    "    total_loss = pos_weight * pos_loss + chunk_weight * chunk_loss + ner_weight * ner_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, device, pos_weight=1.0, chunk_weight=1.0, ner_weight=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training\", unit=\"batch\"):\n",
    "        words, pos_labels, chunk_labels, ner_labels = batch\n",
    "        words = words.to(device)\n",
    "        pos_labels = pos_labels.to(device)\n",
    "        chunk_labels = chunk_labels.to(device)\n",
    "        ner_labels = ner_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pos_out, chunk_out, ner_out = model(words)\n",
    "        loss = compute_loss(pos_out, chunk_out, ner_out, pos_labels, chunk_labels, ner_labels, pos_weight, chunk_weight, ner_weight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, idx2pos, idx2chunk, idx2tag, device):\n",
    "    model.eval()\n",
    "    all_preds_pos, all_labels_pos = [], []\n",
    "    all_preds_chunk, all_labels_chunk = [], []\n",
    "    all_preds_ner, all_labels_ner = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            words, pos_tags, chunk_tags, ner_tags = batch\n",
    "            words = words.to(device)\n",
    "            pos_tags = pos_tags.to(device)\n",
    "            chunk_tags = chunk_tags.to(device)\n",
    "            ner_tags = ner_tags.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(words)\n",
    "            pos_output, chunk_output, ner_output = outputs[0], outputs[1], outputs[2]\n",
    "            \n",
    "            # Convert predictions to labels\n",
    "            pos_preds = torch.argmax(pos_output, dim=-1)\n",
    "            chunk_preds = torch.argmax(chunk_output, dim=-1)\n",
    "            ner_preds = torch.argmax(ner_output, dim=-1)\n",
    "            \n",
    "            all_preds_pos.extend(pos_preds.cpu().numpy().tolist())\n",
    "            all_labels_pos.extend(pos_tags.cpu().numpy().tolist())\n",
    "\n",
    "            all_preds_chunk.extend(chunk_preds.cpu().numpy().tolist())\n",
    "            all_labels_chunk.extend(chunk_tags.cpu().numpy().tolist())\n",
    "\n",
    "            all_preds_ner.extend(ner_preds.cpu().numpy().tolist())\n",
    "            all_labels_ner.extend(ner_tags.cpu().numpy().tolist())\n",
    "    \n",
    "    # Flatten the lists\n",
    "    all_preds_pos = flatten(all_preds_pos)\n",
    "    all_labels_pos = flatten(all_labels_pos)\n",
    "\n",
    "    all_preds_chunk = flatten(all_preds_chunk)\n",
    "    all_labels_chunk = flatten(all_labels_chunk)\n",
    "\n",
    "    all_preds_ner = flatten(all_preds_ner)\n",
    "    all_labels_ner = flatten(all_labels_ner)\n",
    "\n",
    "    # Generate classification reports\n",
    "    pos_report = classification_report(all_labels_pos, all_preds_pos)\n",
    "    chunk_report = classification_report(all_labels_chunk, all_preds_chunk)\n",
    "    ner_report = classification_report(all_labels_ner, all_preds_ner)\n",
    "\n",
    "    return pos_report, chunk_report, ner_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path, vocab):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    \n",
    "    # Save model weights\n",
    "    torch.save(model.state_dict(), f\"{path}.pth\")\n",
    "    \n",
    "    # Save vocab\n",
    "    with open(f\"{path}_vocab.pkl\", \"wb\") as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    \n",
    "    print(f\"Model and vocab saved to {path}.pth and {path}_vocab.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_class, path, device, embed_dim, hidden_dim):\n",
    "    # Load vocab\n",
    "    with open(f\"{path}_vocab.pkl\", \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    \n",
    "    word2idx, pos2idx, chunk2idx, tag2idx = (\n",
    "        vocab[\"word2idx\"],\n",
    "        vocab[\"pos2idx\"],\n",
    "        vocab[\"chunk2idx\"],\n",
    "        vocab[\"tag2idx\"],\n",
    "    )\n",
    "    \n",
    "    # Initialize and load model\n",
    "    model = model_class(\n",
    "        len(word2idx),\n",
    "        len(pos2idx),\n",
    "        len(chunk2idx),\n",
    "        len(tag2idx),\n",
    "        embed_dim,\n",
    "        hidden_dim,\n",
    "    )\n",
    "    model.load_state_dict(torch.load(f\"{path}.pth\", map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Model and vocab loaded successfully.\")\n",
    "    return model, word2idx, pos2idx, chunk2idx, tag2idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text, word2idx, pos2idx, chunk2idx, idx2tag, max_len, device):\n",
    "    words = text.split()\n",
    "    word_indices = [word2idx.get(w, word2idx[\"<UNK>\"]) for w in words]\n",
    "    word_indices = word_indices[:max_len] + [word2idx[\"<PAD>\"]] * (max_len - len(word_indices))\n",
    "    \n",
    "    model_input = torch.tensor([word_indices]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pos_out, chunk_out, ner_out = model(model_input) \n",
    "        predictions = torch.sigmoid(ner_out).cpu().numpy()\n",
    "    \n",
    "    predicted_tags = []\n",
    "    for word_pred in predictions[0][:len(words)]:\n",
    "        max_prob_idx = word_pred.argmax() \n",
    "        predicted_tags.append([idx2tag[max_prob_idx]])\n",
    "    \n",
    "    result = [(w, tags if tags else [\"O\"]) for w, tags in zip(words, predicted_tags)]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "train_file = \"data/eng/eng.train\"\n",
    "val_file = \"data/eng/eng.testa\"\n",
    "test_file = \"data/eng/eng.testb\"\n",
    "\n",
    "# Build vocabulary\n",
    "word2idx, pos2idx, chunk2idx, tag2idx = create_vocab(train_file)\n",
    "idx2pos = {idx: pos for pos, idx in pos2idx.items()}\n",
    "idx2chunk = {idx: chunk for chunk, idx in chunk2idx.items()}\n",
    "idx2tag = {idx: tag for tag, idx in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 1,\n",
       " 'B-MISC': 2,\n",
       " 'B-ORG': 3,\n",
       " 'I-LOC': 4,\n",
       " 'I-MISC': 5,\n",
       " 'I-ORG': 6,\n",
       " 'I-PER': 7,\n",
       " 'O': 0}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "hidden_dim = 256\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = MultiLabelNERDataset(train_file, word2idx, pos2idx, chunk2idx, tag2idx, max_len=50)\n",
    "val_dataset = MultiLabelNERDataset(val_file, word2idx, pos2idx, chunk2idx, tag2idx, max_len=50)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMNERMultiLabel(len(word2idx), len(pos2idx), len(chunk2idx), len(tag2idx), embed_dim, hidden_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:06<00:00, 77.52batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.8617\n",
      "Validation:\n",
      "NER Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98    164812\n",
      "           2       0.00      0.00      0.00         4\n",
      "           4       0.81      0.39      0.52      2088\n",
      "           5       0.68      0.14      0.23      1258\n",
      "           6       0.87      0.20      0.33      2085\n",
      "           7       0.63      0.56      0.59      3053\n",
      "\n",
      "    accuracy                           0.97    173300\n",
      "   macro avg       0.66      0.38      0.44    173300\n",
      "weighted avg       0.96      0.97      0.96    173300\n",
      "\n",
      "Epoch [2/3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:06<00:00, 77.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.3267\n",
      "Validation:\n",
      "NER Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    164812\n",
      "           2       0.00      0.00      0.00         4\n",
      "           4       0.90      0.57      0.70      2088\n",
      "           5       0.89      0.34      0.49      1258\n",
      "           6       0.84      0.41      0.56      2085\n",
      "           7       0.81      0.60      0.69      3053\n",
      "\n",
      "    accuracy                           0.97    173300\n",
      "   macro avg       0.74      0.49      0.57    173300\n",
      "weighted avg       0.97      0.97      0.97    173300\n",
      "\n",
      "Epoch [3/3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:05<00:00, 80.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2177\n",
      "Validation:\n",
      "NER Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    164812\n",
      "           2       0.00      0.00      0.00         4\n",
      "           4       0.90      0.71      0.79      2088\n",
      "           5       0.85      0.58      0.69      1258\n",
      "           6       0.84      0.59      0.69      2085\n",
      "           7       0.85      0.67      0.75      3053\n",
      "\n",
      "    accuracy                           0.98    173300\n",
      "   macro avg       0.74      0.59      0.65    173300\n",
      "weighted avg       0.98      0.98      0.98    173300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "    train_loss = train_model(model, train_loader, optimizer, device)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    print(\"Validation:\")\n",
    "    pos_report, chunk_report, ner_report = evaluate_model(model, val_loader, idx2pos, idx2chunk, idx2tag, device)\n",
    "    print(\"NER Classification Report:\\n\", ner_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vocab saved to save/models/multilabel_bilstm.pth and save/models/multilabel_bilstm_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "save_model(model, \"save/models/multilabel_bilstm\", {\n",
    "    \"word2idx\": word2idx,\n",
    "    \"pos2idx\": pos2idx,\n",
    "    \"chunk2idx\": chunk2idx,\n",
    "    \"tag2idx\": tag2idx,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vocab loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "loaded_model, loaded_word2idx, loaded_pos2idx, loaded_chunk2idx, loaded_tag2idx = load_model(\n",
    "    BiLSTMNERMultiLabel,\n",
    "    \"save/models/multilabel_bilstm\",\n",
    "    device,\n",
    "    embed_dim,\n",
    "    hidden_dim\n",
    ")\n",
    "loaded_idx2tag = {idx: tag for tag, idx in loaded_tag2idx.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', ['O']), ('European', ['I-ORG']), ('Union', ['I-ORG']), ('is', ['O']), ('headquartered', ['O']), ('in', ['O']), ('Brussels', ['I-LOC'])]\n"
     ]
    }
   ],
   "source": [
    "text = \"The European Union is headquartered in Brussels\"\n",
    "predictions = predict(\n",
    "    loaded_model,\n",
    "    text,\n",
    "    loaded_word2idx,\n",
    "    loaded_pos2idx,\n",
    "    loaded_chunk2idx,\n",
    "    loaded_idx2tag,\n",
    "    max_len=50,\n",
    "    device=device\n",
    ")\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
