{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(file_path):\n",
    "    word2idx = defaultdict(lambda: len(word2idx))\n",
    "    tag2idx = defaultdict(lambda: len(tag2idx))\n",
    "    word2idx[\"<PAD>\"] = 0\n",
    "    word2idx[\"<UNK>\"] = 1\n",
    "    # tag2idx[\"O\"] = 0\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                parts = line.strip().split(' ')\n",
    "                word, tag = parts[0], parts[3]\n",
    "                word2idx[word]\n",
    "                tag2idx[tag]\n",
    "    return dict(word2idx), dict(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocab for POS and Chunk\n",
    "def build_aux_vocab(data_file):\n",
    "    pos_vocab, chunk_vocab = {\"<PAD>\": 0, \"<UNK>\": 1}, {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                _, pos, chunk, _ = line.split()\n",
    "                if pos not in pos_vocab:\n",
    "                    pos_vocab[pos] = len(pos_vocab)\n",
    "                if chunk not in chunk_vocab:\n",
    "                    chunk_vocab[chunk] = len(chunk_vocab)\n",
    "    return pos_vocab, chunk_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, file_path, word2idx, tag2idx, pos2idx, chunk2idx, max_len):\n",
    "        self.sentences, self.labels, self.pos_tags, self.chunks = [], [], [], []\n",
    "        sentence, labels, pos_tags, chunks = [], [], [], []\n",
    "\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    word, pos, chunk, tag = line.split()\n",
    "                    sentence.append(word2idx.get(word, word2idx[\"<UNK>\"]))\n",
    "                    labels.append(tag2idx[tag])\n",
    "                    pos_tags.append(pos2idx.get(pos, pos2idx[\"<UNK>\"]))\n",
    "                    chunks.append(chunk2idx.get(chunk, chunk2idx[\"<UNK>\"]))\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        self.sentences.append(self.pad_or_truncate(sentence, max_len))\n",
    "                        self.labels.append(self.pad_or_truncate(labels, max_len))\n",
    "                        self.pos_tags.append(self.pad_or_truncate(pos_tags, max_len))\n",
    "                        self.chunks.append(self.pad_or_truncate(chunks, max_len))\n",
    "                    sentence, labels, pos_tags, chunks = [], [], [], []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = torch.tensor(self.sentences[idx])\n",
    "        labels = torch.tensor(self.labels[idx])\n",
    "        pos_tags = torch.tensor(self.pos_tags[idx])\n",
    "        chunks = torch.tensor(self.chunks[idx])\n",
    "        return sentence, pos_tags, chunks, labels\n",
    "\n",
    "    def pad_or_truncate(self, sequence, max_len):\n",
    "        # Padding\n",
    "        if len(sequence) < max_len:\n",
    "            sequence = sequence + [0] * (max_len - len(sequence))\n",
    "        # Truncation\n",
    "        if len(sequence) > max_len:\n",
    "            sequence = sequence[:max_len]\n",
    "        return sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Pretrained Embeddings and Prepare Dictionaries\n",
    "def load_pretrained_embeddings(file_path, word2idx):\n",
    "    embedding_matrix = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            if word in word2idx:\n",
    "                embedding_matrix.append(vector)\n",
    "    return np.array(embedding_matrix)\n",
    "\n",
    "\n",
    "def load_pretrained_embeddings(file_path, embedding_type='glove'):\n",
    "    embeddings = {}\n",
    "\n",
    "    if embedding_type == 'glove':  # GloVe Format\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "\n",
    "    elif embedding_type == 'word2vec':  # Word2Vec Text Format\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            next(f)  # Skip the first line (header line with vocab size, dimensions)\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "\n",
    "    elif embedding_type == 'fasttext':  # FastText Format\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            next(f)  # Skip the first line (header line)\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported embedding type: {embedding_type}\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def create_embedding_matrix(vocab, pretrained_embeddings, embedding_dim, unk_token='<UNK>'):\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, idx in vocab.items():\n",
    "        if word in pretrained_embeddings:\n",
    "            embedding_matrix[idx] = pretrained_embeddings[word]\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.normal(size=(embedding_dim,))  # Random for unknowns\n",
    "\n",
    "    # Handle the unknown token if present\n",
    "    if unk_token in vocab:\n",
    "        embedding_matrix[vocab[unk_token]] = np.random.normal(size=(embedding_dim,))\n",
    "\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMNER(nn.Module):\n",
    "    def __init__(self, vocab_size, pos_size, chunk_size, tagset_size, embed_dim, pos_dim, chunk_dim, hidden_dim):\n",
    "        super(BiLSTMNER, self).__init__()\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embeds = nn.Embedding(pos_size, pos_dim)\n",
    "        self.chunk_embeds = nn.Embedding(chunk_size, chunk_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim + pos_dim + chunk_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "\n",
    "    def forward(self, words, pos_tags, chunks):\n",
    "        word_embeds = self.word_embeds(words)\n",
    "        pos_embeds = self.pos_embeds(pos_tags)\n",
    "        chunk_embeds = self.chunk_embeds(chunks)\n",
    "        combined = torch.cat((word_embeds, pos_embeds, chunk_embeds), dim=-1)\n",
    "        lstm_out, _ = self.lstm(combined)\n",
    "        logits = self.fc(lstm_out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\", unit=\"batch\"):\n",
    "        words, pos_tags, chunks, labels = batch\n",
    "        \n",
    "        words = words.to(device)\n",
    "        pos_tags = pos_tags.to(device)\n",
    "        chunks = chunks.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(words, pos_tags, chunks)\n",
    "        \n",
    "        # Flatten outputs and labels for loss calculation\n",
    "        loss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, idx2tag, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            words, pos_tags, chunks, labels = batch\n",
    "            words = words.to(device)\n",
    "            pos_tags = pos_tags.to(device)\n",
    "            chunks = chunks.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(words, pos_tags, chunks)\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            all_preds.extend(predictions.view(-1).tolist())\n",
    "            all_labels.extend(labels.view(-1).tolist())\n",
    "    \n",
    "    # Remove padding tokens\n",
    "    valid_preds = []\n",
    "    valid_labels = []\n",
    "    \n",
    "    for p, l in zip(all_preds, all_labels):\n",
    "        # Kiểm tra xem nhãn có trong idx2tag không trước khi sử dụng\n",
    "        # if l in idx2tag and idx2tag[l] != \"O\":\n",
    "        valid_preds.append(idx2tag[p])\n",
    "        valid_labels.append(idx2tag[l])\n",
    "    \n",
    "    return classification_report(valid_labels, valid_preds, output_dict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path, word2idx, pos2idx, chunk2idx, tag2idx):\n",
    "    torch.save(model.state_dict(), f\"{path}.pth\")\n",
    "    with open(f\"{path}_vocab.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"word2idx\": word2idx, \"pos2idx\": pos2idx, \"chunk2idx\": chunk2idx, \"tag2idx\": tag2idx}, f)\n",
    "    print(f\"Model and vocab saved to {path}.pth and {path}_vocab.pkl\")\n",
    "\n",
    "def load_model(model_class, path, vocab_path, embed_dim, pos_dim, chunk_dim, hidden_dim, device):\n",
    "    # Load vocab\n",
    "    with open(vocab_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    word2idx, pos2idx, chunk2idx, tag2idx = vocab[\"word2idx\"], vocab[\"pos2idx\"], vocab[\"chunk2idx\"], vocab[\"tag2idx\"]\n",
    "    \n",
    "    # Khởi tạo lại mô hình với đầy đủ tham số\n",
    "    model = model_class(len(word2idx), len(pos2idx), len(chunk2idx), len(tag2idx), embed_dim, pos_dim, chunk_dim, hidden_dim)\n",
    "    \n",
    "    # Load state_dict\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    \n",
    "    return model, word2idx, pos2idx, chunk2idx, tag2idx\n",
    "\n",
    "\n",
    "def predict(model, text, word2idx, pos2idx, chunk2idx, idx2tag, max_len, device):\n",
    "    words = text.split()\n",
    "    # Map words to indices\n",
    "    word_indices = [word2idx.get(w, word2idx[\"<PAD>\"]) for w in words]\n",
    "    word_indices = word_indices[:max_len] + [word2idx[\"<PAD>\"]] * (max_len - len(word_indices))\n",
    "\n",
    "    # Generate dummy POS tags and chunks (e.g., assuming \"NN\" and \"B-NP\" for simplicity)\n",
    "    pos_indices = [pos2idx.get(\"NN\", pos2idx[\"<PAD>\"])] * len(words)\n",
    "    chunk_indices = [chunk2idx.get(\"B-NP\", chunk2idx[\"<PAD>\"])] * len(words)\n",
    "\n",
    "    # Pad POS and chunks\n",
    "    pos_indices = pos_indices[:max_len] + [pos2idx[\"<PAD>\"]] * (max_len - len(pos_indices))\n",
    "    chunk_indices = chunk_indices[:max_len] + [chunk2idx[\"<PAD>\"]] * (max_len - len(chunk_indices))\n",
    "\n",
    "    # Prepare tensors\n",
    "    word_tensor = torch.tensor([word_indices]).to(device)\n",
    "    pos_tensor = torch.tensor([pos_indices]).to(device)\n",
    "    chunk_tensor = torch.tensor([chunk_indices]).to(device)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(word_tensor, pos_tensor, chunk_tensor)\n",
    "        predictions = torch.argmax(outputs, dim=-1).squeeze(0).tolist()\n",
    "\n",
    "    # Convert predictions to tags\n",
    "    tags = [idx2tag[idx] for idx in predictions[:len(words)]]\n",
    "    return list(zip(words, tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "train_file = \"data/eng/eng.train\"\n",
    "val_file = \"data/eng/eng.testa\"\n",
    "test_file = \"data/eng/eng.testb\"\n",
    "\n",
    "# Build vocabulary\n",
    "pos2idx, chunk2idx = build_aux_vocab(\"data/eng/eng.train\")\n",
    "word2idx, tag2idx = build_vocab(train_file)\n",
    "idx2tag = {idx: tag for tag, idx in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<UNK>': 1,\n",
       " 'NNP': 2,\n",
       " 'VBZ': 3,\n",
       " 'JJ': 4,\n",
       " 'NN': 5,\n",
       " 'TO': 6,\n",
       " 'VB': 7,\n",
       " '.': 8,\n",
       " 'CD': 9,\n",
       " 'DT': 10,\n",
       " 'VBD': 11,\n",
       " 'IN': 12,\n",
       " 'PRP': 13,\n",
       " 'NNS': 14,\n",
       " 'VBP': 15,\n",
       " 'MD': 16,\n",
       " 'VBN': 17,\n",
       " 'POS': 18,\n",
       " 'JJR': 19,\n",
       " '\"': 20,\n",
       " 'RB': 21,\n",
       " ',': 22,\n",
       " 'FW': 23,\n",
       " 'CC': 24,\n",
       " 'WDT': 25,\n",
       " '(': 26,\n",
       " ')': 27,\n",
       " ':': 28,\n",
       " 'PRP$': 29,\n",
       " 'RBR': 30,\n",
       " 'VBG': 31,\n",
       " 'EX': 32,\n",
       " 'WP': 33,\n",
       " 'WRB': 34,\n",
       " '-X-': 35,\n",
       " '$': 36,\n",
       " 'RP': 37,\n",
       " 'NNPS': 38,\n",
       " 'SYM': 39,\n",
       " 'RBS': 40,\n",
       " 'UH': 41,\n",
       " 'PDT': 42,\n",
       " \"''\": 43,\n",
       " 'LS': 44,\n",
       " 'JJS': 45,\n",
       " 'WP$': 46,\n",
       " 'NN|SYM': 47}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_dim = 100\n",
    "pos_dim = 50\n",
    "chunk_dim = 50\n",
    "hidden_dim = 128\n",
    "max_len = 50\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(train_file, word2idx, tag2idx, pos2idx, chunk2idx, max_len)\n",
    "val_dataset = NERDataset(val_file, word2idx, tag2idx, pos2idx, chunk2idx, max_len)\n",
    "test_dataset = NERDataset(test_file, word2idx, tag2idx, pos2idx, chunk2idx, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate class weights\n",
    "# num_tags = len(tag2idx)\n",
    "# tag_counts = [0] * num_tags\n",
    "# for _, labels in train_dataset:\n",
    "#     for tag in labels.tolist():\n",
    "#         tag_counts[tag] += 1\n",
    "# total_tags = sum(tag_counts)\n",
    "# class_weights = [total_tags / count if count > 0 else 0.0 for count in tag_counts]\n",
    "\n",
    "# # Convert to tensor and move to device\n",
    "# weights = torch.tensor(class_weights).to(device)\n",
    "\n",
    "# # Define loss function with weights\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=0, weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, ignore_index=-1):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.CrossEntropyLoss(ignore_index=self.ignore_index, reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)  # Probabilities of the true class\n",
    "        focal_loss = self.alpha * ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Replace criterion with FocalLoss\n",
    "criterion = FocalLoss(alpha=1, gamma=2, ignore_index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, optimizer, loss\n",
    "model = BiLSTMNER(len(word2idx), len(pos2idx), len(chunk2idx), len(tag2idx), embed_dim, pos_dim, chunk_dim, hidden_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:02<00:00, 157.57batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0253\n",
      "Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       I-LOC       0.64      0.78      0.70      2088\n",
      "      I-MISC       0.50      0.66      0.57      1258\n",
      "       I-ORG       0.00      0.00      0.00    124126\n",
      "       I-PER       0.02      0.84      0.04      3053\n",
      "           O       0.76      0.99      0.86     42721\n",
      "\n",
      "    accuracy                           0.27    173250\n",
      "   macro avg       0.32      0.55      0.36    173250\n",
      "weighted avg       0.20      0.27      0.23    173250\n",
      "\n",
      "===================================================================================\n",
      "Epoch [2/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:02<00:00, 172.22batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0090\n",
      "Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       I-LOC       0.70      0.86      0.77      2088\n",
      "      I-MISC       0.66      0.73      0.69      1258\n",
      "       I-ORG       0.00      0.00      0.00    124126\n",
      "       I-PER       0.23      0.89      0.36      3053\n",
      "           O       0.27      0.99      0.42     42721\n",
      "\n",
      "    accuracy                           0.28    173250\n",
      "   macro avg       0.31      0.58      0.38    173250\n",
      "weighted avg       0.08      0.28      0.13    173250\n",
      "\n",
      "===================================================================================\n",
      "Epoch [3/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:02<00:00, 176.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0048\n",
      "Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       I-LOC       0.74      0.84      0.79      2088\n",
      "      I-MISC       0.68      0.75      0.71      1258\n",
      "       I-ORG       0.00      0.00      0.00    124126\n",
      "       I-PER       0.02      0.93      0.05      3053\n",
      "           O       0.84      0.99      0.91     42721\n",
      "\n",
      "    accuracy                           0.28    173250\n",
      "   macro avg       0.38      0.59      0.41    173250\n",
      "weighted avg       0.22      0.28      0.24    173250\n",
      "\n",
      "===================================================================================\n",
      "Epoch [4/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:02<00:00, 177.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0024\n",
      "Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       I-LOC       0.79      0.84      0.82      2088\n",
      "      I-MISC       0.57      0.84      0.68      1258\n",
      "       I-ORG       0.00      0.00      0.00    124126\n",
      "       I-PER       0.02      0.90      0.05      3053\n",
      "           O       0.78      0.99      0.87     42721\n",
      "\n",
      "    accuracy                           0.28    173250\n",
      "   macro avg       0.36      0.60      0.40    173250\n",
      "weighted avg       0.21      0.28      0.23    173250\n",
      "\n",
      "===================================================================================\n",
      "Epoch [5/5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 469/469 [00:02<00:00, 175.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0010\n",
      "Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-MISC       0.00      0.00      0.00         4\n",
      "       I-LOC       0.72      0.89      0.79      2088\n",
      "      I-MISC       0.66      0.79      0.72      1258\n",
      "       I-ORG       0.00      0.00      0.00    124126\n",
      "       I-PER       0.02      0.92      0.05      3053\n",
      "           O       0.89      0.99      0.94     42721\n",
      "\n",
      "    accuracy                           0.28    173250\n",
      "   macro avg       0.38      0.60      0.42    173250\n",
      "weighted avg       0.23      0.28      0.25    173250\n",
      "\n",
      "===================================================================================\n",
      "Final Test Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "      B-MISC       0.00      0.00      0.00         9\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-LOC       0.67      0.86      0.75      1905\n",
      "      I-MISC       0.48      0.71      0.57       908\n",
      "       I-ORG       0.00      0.00      0.00    140249\n",
      "       I-PER       0.02      0.91      0.04      2691\n",
      "           O       0.85      0.99      0.91     38377\n",
      "\n",
      "    accuracy                           0.23    184150\n",
      "   macro avg       0.25      0.43      0.28    184150\n",
      "weighted avg       0.19      0.23      0.20    184150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and validate\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    print(\"Validation:\")\n",
    "    val_report = evaluate_model(model, val_loader, idx2tag, device)\n",
    "    print(val_report)\n",
    "    print(\"===================================================================================\")\n",
    "\n",
    "# Final test evaluation\n",
    "print(\"Final Test Evaluation:\")\n",
    "test_report = evaluate_model(model, test_loader, idx2tag, device)\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vocab saved to save/models/bilstm_3f1l.pth and save/models/bilstm_3f1l_vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model_path = \"save/models/bilstm_3f1l\"\n",
    "save_model(model, model_path, word2idx, pos2idx, chunk2idx, tag2idx)\n",
    "\n",
    "# Load model and predict\n",
    "# Load model and predict\n",
    "loaded_model, loaded_word2idx, loaded_pos2idx, loaded_chunk2idx, loaded_tag2idx = load_model(\n",
    "    BiLSTMNER, f\"{model_path}.pth\", f\"{model_path}_vocab.pkl\", embed_dim, pos_dim, chunk_dim, hidden_dim, device\n",
    ")\n",
    "loaded_idx2tag = {idx: tag for tag, idx in loaded_tag2idx.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'O'), ('European', 'I-MISC'), ('Union', 'O'), ('is', 'O'), ('headquartered', 'O'), ('in', 'O'), ('Brussels', 'O')]\n"
     ]
    }
   ],
   "source": [
    "text = \"The European Union is headquartered in Brussels\"\n",
    "result = predict(model, text, word2idx, pos2idx, chunk2idx, idx2tag, max_len=50, device=device)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
