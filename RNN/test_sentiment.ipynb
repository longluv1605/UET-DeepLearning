{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import requirments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter        \n",
    "\n",
    "LANGUAGE='english'\n",
    "\n",
    "\n",
    "# Load requirements\n",
    "def load_requirements():\n",
    "    # Init neccessary tools\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"punkt_tab\")\n",
    "    nltk.download(\"stopwords\")\n",
    "    nltk.download(\"wordnet\")\n",
    "\n",
    "# Sentence preprocessing function\n",
    "def preprocess_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words(LANGUAGE))\n",
    "    \n",
    "    text = text.lower()  # Convert into lower case\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [\n",
    "        word for word in tokens if word not in string.punctuation\n",
    "    ]  # Remove punctuation\n",
    "    tokens = [\n",
    "        word for word in tokens if word not in stop_words\n",
    "    ]  # Remove stopwords\n",
    "    tokens = [stemmer.stem(word) for word in tokens]  # Stemming\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lematizing\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Build vocab\n",
    "def build_vocab(texts, max_vocab_size=10000):\n",
    "    word_counts = Counter()\n",
    "    for text in texts:\n",
    "        word_counts.update(text.split())\n",
    "    vocab = {\n",
    "        word: idx + 2\n",
    "        for idx, (word, _) in enumerate(word_counts.most_common(max_vocab_size))\n",
    "    }\n",
    "    vocab[\"<PAD>\"] = 0\n",
    "    vocab[\"<UNK>\"] = 1\n",
    "    return vocab\n",
    "\n",
    "# Indexing\n",
    "def encode_text(text, vocab):\n",
    "    return [vocab.get(word, vocab[\"<UNK>\"]) for word in text.split()]\n",
    "\n",
    "# Encode padding for train and test set\n",
    "def pad_sequences(sequences, max_length):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > max_length:\n",
    "            padded_sequences.append(seq[:max_length])\n",
    "        elif len(seq) < max_length:\n",
    "            padded_sequences.append(seq + [0] * (max_length - len(seq)))\n",
    "        else:\n",
    "            padded_sequences.append(seq)\n",
    "    return torch.Tensor(padded_sequences)\n",
    "\n",
    "def load_dataset(dataframe, max_length):\n",
    "    # Split data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataframe[\"review\"], dataframe[\"sentiment\"], test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Build vocab\n",
    "    vocab = build_vocab(X_train)\n",
    "\n",
    "    # Encode and padding\n",
    "    X_train_encoded = [encode_text(text, vocab) for text in X_train]\n",
    "    X_test_encoded = [encode_text(text, vocab) for text in X_test]\n",
    "    X_train_padded = pad_sequences(X_train_encoded, max_length)\n",
    "    X_test_padded = pad_sequences(X_test_encoded, max_length)\n",
    "\n",
    "    # Convert into Tensor\n",
    "    y_train = torch.tensor([1 if label == \"positive\" else 0 for label in y_train])\n",
    "    y_test = torch.tensor([1 if label == \"positive\" else 0 for label in y_test])\n",
    "    \n",
    "    train_dataset = TextDataset(X_train_padded, y_train)\n",
    "    test_dataset = TextDataset(X_test_padded, y_test)\n",
    "\n",
    "    return vocab, train_dataset, test_dataset\n",
    "\n",
    "\n",
    "# Define dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts.long()\n",
    "        self.labels = labels.long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/imdb/review.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 256  # Max length of sentence to encode\n",
    "\n",
    "vocab, train_dataset, test_dataset = load_dataset(df, MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Stacked RNN"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
