{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchmetrics\n",
    "import logging\n",
    "from itertools import product\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_hidden_nodes, n_classes, image_width=32, image_height=32, color_channels=3, n_hidden_layers=1):\n",
    "        super(MLP, self).__init__()\n",
    "        input_size = image_width * image_height * color_channels\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden_nodes, n_classes)\n",
    "        )\n",
    "        \n",
    "        if n_hidden_layers > 1:\n",
    "            self.added_layers = nn.Sequential()\n",
    "            for i in range(n_hidden_layers - 1):\n",
    "                self.added_layers.add_module(str(2 * (i + 1) + 1), nn.Linear(n_hidden_nodes, n_hidden_nodes))\n",
    "                self.added_layers.add_module(str(2 * (i + 1) + 2), nn.ReLU())\n",
    "            layers = list(self.layers)\n",
    "            layers.insert(2, self.added_layers)\n",
    "            self.layers = nn.Sequential(*layers)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class DataLoaderFactory:\n",
    "    def __init__(self, root='./data', transform=None, batch_size=32, num_workers=2, download=True):\n",
    "        self.root = root\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.download = download\n",
    "\n",
    "    def load_data(self, train=True):\n",
    "        dataset = torchvision.datasets.CIFAR10(\n",
    "            root=self.root, \n",
    "            train=train, \n",
    "            download=self.download, \n",
    "            transform=self.transform\n",
    "        )\n",
    "        return DataLoader(\n",
    "            dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=train, \n",
    "            num_workers=self.num_workers\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define Model trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model=None, criterion=None, optimizer=None, dataloader=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model or MLP(n_classes=10, n_hidden_nodes=100, image_width=32, image_height=32, color_channels=3)\n",
    "        self.model.to(self.device)\n",
    "        self.criterion = criterion or CrossEntropyLoss()\n",
    "        self.optimizer = optimizer or SGD(self.model.parameters(), lr=0.005)\n",
    "        self.dataloader = dataloader or DataLoaderFactory(root='./data', batch_size=32, num_workers=4, download=False)\n",
    "        self.epochs = 0\n",
    "\n",
    "    def train(self, epochs=10, write_log=False):\n",
    "        self.epochs = epochs\n",
    "        train_loader = self.dataloader.load_data(train=True)\n",
    "        train_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(self.device)\n",
    "        test_loader = self.dataloader.load_data(train=False)\n",
    "        test_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(self.device)\n",
    "\n",
    "        if write_log:\n",
    "            num_layers = len(list(self.model.children())) // 2 + 1\n",
    "            id = random.randint(0, 1000)\n",
    "            logging.basicConfig(filename=f'training_mlp_{num_layers}_hidden_layers_{id}.log', level=logging.INFO)\n",
    "            logging.info(\"Training started\\n\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            self.model.train()\n",
    "            train_accuracy.reset()\n",
    "\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                train_accuracy.update(outputs.argmax(dim=1), labels)\n",
    "\n",
    "            final_train_accuracy = train_accuracy.compute()\n",
    "            print(f'Epoch [{epoch+1}/{epochs}]\\n', \n",
    "                  f'Loss: {running_loss/len(train_loader):.4f}\\n',\n",
    "                  f'Train Accuracy: {final_train_accuracy * 100:.2f}\\n',\n",
    "                  '--------------------------------------------------\\n')\n",
    "\n",
    "            final_test_accuracy = self.evaluate(test_loader=test_loader, test_accuracy=test_accuracy)\n",
    "            if write_log:\n",
    "                logging.info(f\"Epoch: {epoch + 1}, Loss: {running_loss/len(train_loader):.4f}, Train accuracy: {final_train_accuracy}\\t|\\t Test accuracy: {final_test_accuracy}\\n\")\n",
    "\n",
    "        print('======================Finished=========================')\n",
    "        return running_loss / len(train_loader)\n",
    "\n",
    "    def evaluate(self, test_loader=None, test_accuracy=None):\n",
    "        test_loader = test_loader or self.dataloader.load_data(train=False)\n",
    "        test_accuracy = test_accuracy or torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(self.device)\n",
    "        self.model.eval()\n",
    "        test_accuracy.reset()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                test_accuracy.update(outputs.argmax(dim=1), labels)\n",
    "\n",
    "        final_test_accuracy = test_accuracy.compute()\n",
    "        print(f'Test Accuracy: {final_test_accuracy * 100:.2f}\\n',\n",
    "              '--------------------------------------------------\\n')\n",
    "        return final_test_accuracy\n",
    "\n",
    "    def predict(self, data):\n",
    "        predictions = []\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for input in data:\n",
    "                input = input.to(self.device)\n",
    "                outputs = self.model(input)\n",
    "                predictions.append(outputs.argmax(dim=1))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def save(self, name=None):\n",
    "        parent = 'models'\n",
    "        checkpoint_path = str(name) + '.pth' or 'mlp_checkpoint.pth'\n",
    "        path = f'{parent}/{checkpoint_path}'\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'criterion_state_dict': self.criterion.state_dict(),\n",
    "            'epochs': self.epochs\n",
    "        }\n",
    "        torch.save(checkpoint, path)\n",
    "        print(f\"Checkpoint saved to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning(model, epochs=3, write_log=False, download=True):\n",
    "    param_grid = {\n",
    "        'batch_size': [16, 32, 64],\n",
    "        'learning_rate': [1e-3, 5e-3],\n",
    "        'optimizer': ['SGD', 'Adam']\n",
    "    }\n",
    "\n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "\n",
    "    best_params = None\n",
    "    best_loss = float('inf')\n",
    "    for params in param_combinations:\n",
    "        batch_size, lr, opt = params\n",
    "        print(f\"\\nTesting with batch size={batch_size}, learning rate={lr}, optimizer={opt}\")\n",
    "        \n",
    "        dataloader = DataLoaderFactory(root='./data', batch_size=batch_size, download=download)\n",
    "\n",
    "        optimizer = SGD(model.parameters(), lr=lr) if opt == 'SGD' else Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        model_trainer = ModelTrainer(model=model, dataloader=dataloader, optimizer=optimizer)\n",
    "        avg_loss = model_trainer.train(epochs=epochs, write_log=write_log)\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_params = {\n",
    "                'batch_size': batch_size,\n",
    "                'learning_rate': lr,\n",
    "                'optimizer': opt\n",
    "            }\n",
    "\n",
    "    return best_params, best_loss, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. MLP with 1 hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1. Random choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hlayer_model = MLP(n_classes=10, n_hidden_nodes=100, image_width=32, image_height=32, color_channels=3)\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = SGD(one_hlayer_model.parameters(), lr=0.005)\n",
    "\n",
    "model_trainer = ModelTrainer(model=one_hlayer_model, criterion=criterion, optimizer=optimizer)\n",
    "\n",
    "model_trainer.train(epochs=20, write_log=True)\n",
    "\n",
    "model_trainer.save('one_hidden_model_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2. Hyper tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hlayer_model_tuning = MLP(n_classes=10, n_hidden_nodes=100, image_width=32, image_height=32, color_channels=3)\n",
    "\n",
    "best_params, best_lost, one_hlayer_model_tuning = hyper_tuning(model=one_hlayer_model_tuning, epochs=10, write_log=True, download=False)\n",
    "\n",
    "print('Best hyperparameters: ', best_params)\n",
    "print('Best loss: ', best_lost)\n",
    "\n",
    "torch.save(best_params, '1h_best_hparams.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. MLP with 2 hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1. Random choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_hlayer_model = MLP(n_classes=10, n_hidden_nodes=100, image_width=32, image_height=32, color_channels=3, n_hidden_layers=2)\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = SGD(one_hlayer_model.parameters(), lr=0.005)\n",
    "\n",
    "model_trainer = ModelTrainer(model=one_hlayer_model, criterion=criterion, optimizer=optimizer)\n",
    "\n",
    "model_trainer.train(epochs=20, write_log=True)\n",
    "\n",
    "model_trainer.save('two_hidden_model_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2. Hyper tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_hlayer_model_tuning = MLP(n_classes=10, n_hidden_nodes=100, image_width=32, image_height=32, color_channels=3, n_hidden_layers=2)\n",
    "\n",
    "best_params, best_lost, one_hlayer_model_tuning = hyper_tuning(model=one_hlayer_model_tuning, epochs=10, write_log=True, download=False)\n",
    "\n",
    "print('Best hyperparameters: ', best_params)\n",
    "print('Best loss: ', best_lost)\n",
    "\n",
    "torch.save(best_params, '2h_best_hparams.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
