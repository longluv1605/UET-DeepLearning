{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchmetrics\n",
    "import logging\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_hidden_nodes, n_classes, image_width=32, image_height=32, color_channels=3, n_hidden_layers=1):\n",
    "        super(MLP, self).__init__()\n",
    "        input_size = image_width * image_height * color_channels\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden_nodes, n_classes)\n",
    "        )\n",
    "        \n",
    "        if n_hidden_layers > 1:\n",
    "            self.added_layers = nn.Sequential()\n",
    "            for i in range(n_hidden_layers - 1):\n",
    "                self.added_layers.add_module(str(2 * (i + 1) + 1), nn.Linear(n_hidden_nodes, n_hidden_nodes))\n",
    "                self.added_layers.add_module(str(2 * (i + 1) + 2), nn.ReLU())\n",
    "            layers = list(self.layers)\n",
    "            layers.insert(2, self.added_layers)\n",
    "            self.layers = nn.Sequential(*layers)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DataLoaderFactory:\n",
    "    def __init__(self, root='./data', transform=None, batch_size=32, num_workers=2):\n",
    "        self.root = root\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def load_data(self, train=True):\n",
    "        dataset = torchvision.datasets.CIFAR10(\n",
    "            root=self.root, \n",
    "            train=train, \n",
    "            download=True, \n",
    "            transform=self.transform\n",
    "        )\n",
    "        return DataLoader(\n",
    "            dataset, \n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=train, \n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "# Usage\n",
    "data_loader_factory = DataLoaderFactory()\n",
    "train_loader = data_loader_factory.load_data(train=True)\n",
    "test_loader = data_loader_factory.load_data(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "import logging\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, model=None, criterion=None, optimizer=None, dataloader=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model or MLP(n_classes=10, n_hidden_nodes=100, image_width=32, image_height=32, color_channels=3)\n",
    "        self.model.to(self.device)\n",
    "        self.criterion = criterion or CrossEntropyLoss()\n",
    "        self.optimizer = optimizer or SGD(self.model.parameters(), lr=0.005)\n",
    "        self.dataloader = dataloader or DataLoaderFactory(root='./data', batch_size=32, num_workers=4)\n",
    "        self.epochs = 0\n",
    "\n",
    "    def train(self, epochs=10, write_log=False):\n",
    "        self.epochs = epochs\n",
    "        train_loader = self.dataloader.load_data(train=True)\n",
    "        train_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(self.device)\n",
    "        test_loader = self.dataloader.load_data(train=False)\n",
    "        test_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(self.device)\n",
    "\n",
    "        if write_log:\n",
    "            num_layers = len(list(self.model.children())) // 2 + 1\n",
    "            logging.basicConfig(filename=f'training_mlp_{num_layers}_hidden_layers.log', level=logging.INFO)\n",
    "            logging.info(\"Training started\\n\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            self.model.train()\n",
    "            train_accuracy.reset()\n",
    "\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                train_accuracy.update(outputs.argmax(dim=1), labels)\n",
    "\n",
    "            final_train_accuracy = train_accuracy.compute()\n",
    "            print(f'Epoch [{epoch+1}/{epochs}]\\n', \n",
    "                  f'Loss: {running_loss/len(train_loader):.4f}\\n',\n",
    "                  f'Train Accuracy: {final_train_accuracy * 100:.2f}\\n',\n",
    "                  '--------------------------------------------------\\n')\n",
    "\n",
    "            final_test_accuracy = self.evaluate(test_loader=test_loader, test_accuracy=test_accuracy)\n",
    "            if write_log:\n",
    "                logging.info(f\"Epoch: {epoch + 1}, Loss: {running_loss/len(train_loader):.4f}, Train accuracy: {final_train_accuracy}\\t|\\t Test accuracy: {final_test_accuracy}\\n\")\n",
    "\n",
    "        print('======================Finished=========================')\n",
    "        return running_loss / len(train_loader)\n",
    "\n",
    "    def evaluate(self, test_loader=None, test_accuracy=None):\n",
    "        test_loader = test_loader or self.dataloader.load_data(train=False)\n",
    "        test_accuracy = test_accuracy or torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(self.device)\n",
    "        self.model.eval()\n",
    "        test_accuracy.reset()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                test_accuracy.update(outputs.argmax(dim=1), labels)\n",
    "\n",
    "        final_test_accuracy = test_accuracy.compute()\n",
    "        print(f'Test Accuracy: {final_test_accuracy * 100:.2f}\\n',\n",
    "              '--------------------------------------------------\\n')\n",
    "        return final_test_accuracy\n",
    "\n",
    "    def predict(self, data):\n",
    "        predictions = []\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for input in data:\n",
    "                input = input.to(self.device)\n",
    "                outputs = self.model(input)\n",
    "                predictions.append(outputs.argmax(dim=1))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def save(self, name=None):\n",
    "        parent = 'models'\n",
    "        checkpoint_path = name or 'mlp_checkpoint.pth'\n",
    "        path = f'{parent}/{checkpoint_path}'\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'criterion_state_dict': self.criterion.state_dict(),\n",
    "            'epochs': self.epochs\n",
    "        }\n",
    "        torch.save(checkpoint, path)\n",
    "        print(f\"Checkpoint saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/1]\n",
      " Loss: 1.9885\n",
      " Train Accuracy: 28.82\n",
      " --------------------------------------------------\n",
      "\n",
      "Test Accuracy: 34.21\n",
      " --------------------------------------------------\n",
      "\n",
      "======================Finished=========================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9885345200888256"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Model()\n",
    "clf.train(epochs=1, write_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_tuning(model, epochs=1):\n",
    "    param_grid = {\n",
    "        'batch_size': [16, 32, 64],\n",
    "        'learning_rate': [1e-3, 5e-3],\n",
    "        'optimizer': ['SGD', 'Adam']\n",
    "    }\n",
    "\n",
    "    param_grid = {\n",
    "        'batch_size': [16],\n",
    "        'learning_rate': [5e-3],\n",
    "        'optimizer': ['Adam']\n",
    "    }\n",
    "\n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "\n",
    "    best_params = None\n",
    "    best_loss = float('inf')\n",
    "    for params in param_combinations:\n",
    "        batch_size, lr, opt = params\n",
    "        print(f\"\\nTesting with batch size={batch_size}, learning rate={lr}, optimizer={opt}\")\n",
    "        \n",
    "        dataloader = DataLoaderFactory(root='./data', batch_size=batch_size)\n",
    "\n",
    "        if opt == 'SGD':\n",
    "            optimizer = SGD(model.parameters(), lr=lr)\n",
    "        elif opt == 'Adam':\n",
    "            optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        clf = Model(model=model, dataloader=dataloader, optimizer=optimizer)\n",
    "        avg_loss = clf.train(epochs=epochs)\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_params = {\n",
    "                'batch_size': batch_size,\n",
    "                'learning_rate': lr,\n",
    "                'optimizer': opt\n",
    "            }\n",
    "\n",
    "    return best_params, best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(n_classes=10, n_hidden_nodes=100, image_width=32, image_height=32, color_channels=3)\n",
    "\n",
    "hyper_tuning(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.save('mlp_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the refactored code into a Python file as requested.\n",
    "\n",
    "# refactored_code = \"\"\"\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchmetrics\n",
    "import logging\n",
    "from itertools import product\n",
    "\n",
    "# Define MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_hidden_nodes, n_classes, image_width=32, image_height=32, color_channels=3, n_hidden_layers=1):\n",
    "        super(MLP, self).__init__()\n",
    "        input_size = image_width * image_height * color_channels\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden_nodes, n_classes)\n",
    "        )\n",
    "        \n",
    "        if n_hidden_layers != 1:\n",
    "            self.added_layers = nn.Sequential()\n",
    "            for i in range(n_hidden_layers - 1):\n",
    "                self.added_layers.add_module(str(2 * (i + 1) + 1), nn.Linear(n_hidden_nodes, n_hidden_nodes))\n",
    "                self.added_layers.add_module(str(2 * (i + 1) + 2), nn.ReLU())\n",
    "            layers = list(self.layers)\n",
    "            layers.insert(2, self.added_layers)\n",
    "            self.layers = nn.Sequential(*layers)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# Data loading utility\n",
    "class Data:\n",
    "    def __init__(self, root='./data', batch_size=32, num_workers=2):\n",
    "        self.root = root\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    def load_data(self, train=True):\n",
    "        dataset = torchvision.datasets.CIFAR10(root=self.root, train=train, download=True, transform=self.transform)\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=train, num_workers=self.num_workers)\n",
    "\n",
    "# Model training, evaluation, and saving\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, dataloader, optimizer, criterion=CrossEntropyLoss(), epochs=10, log_path=None):\n",
    "        self.model = model.to(self._get_device())\n",
    "        self.dataloader = dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.epochs = epochs\n",
    "        self.device = self._get_device()\n",
    "        if log_path:\n",
    "            logging.basicConfig(filename=log_path, level=logging.INFO)\n",
    "    \n",
    "    def _get_device(self):\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def train(self):\n",
    "        train_loader = self.dataloader.load_data(train=True)\n",
    "        accuracy_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(self.device)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0.0\n",
    "            self.model.train()\n",
    "            accuracy_metric.reset()\n",
    "\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                accuracy_metric.update(outputs.argmax(dim=1), labels)\n",
    "\n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "            avg_accuracy = accuracy_metric.compute().item() * 100\n",
    "            print(f'Epoch [{epoch+1}/{self.epochs}], Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.2f}%')\n",
    "            if logging.getLogger().isEnabledFor(logging.INFO):\n",
    "                logging.info(f\"Epoch {epoch+1}/{self.epochs}: Loss={avg_loss:.4f}, Accuracy={avg_accuracy:.2f}%\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        test_loader = self.dataloader.load_data(train=False)\n",
    "        accuracy_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                accuracy_metric.update(outputs.argmax(dim=1), labels)\n",
    "\n",
    "        accuracy = accuracy_metric.compute().item() * 100\n",
    "        print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "        return accuracy\n",
    "\n",
    "    def save_model(self, path='mlp_checkpoint.pth'):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        print(f'Model saved to {path}')\n",
    "\n",
    "# Hyperparameter tuning function\n",
    "def hyperparameter_tuning(model, param_grid, dataloader, epochs=1):\n",
    "    best_params = None\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for batch_size, lr, opt_name in product(*param_grid.values()):\n",
    "        print(f'\\\\nTesting with batch_size={batch_size}, lr={lr}, optimizer={opt_name}')\n",
    "        \n",
    "        optimizer = SGD(model.parameters(), lr=lr) if opt_name == 'SGD' else Adam(model.parameters(), lr=lr)\n",
    "        dataloader.batch_size = batch_size\n",
    "        trainer = ModelTrainer(model, dataloader, optimizer, epochs=epochs)\n",
    "        \n",
    "        loss = trainer.train()\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_params = {'batch_size': batch_size, 'lr': lr, 'optimizer': opt_name}\n",
    "\n",
    "    print(f'Best params: {best_params}, Loss: {best_loss:.4f}')\n",
    "    return best_params\n",
    "\n",
    "# # Execution script\n",
    "# if __name__ == \"__main__\":\n",
    "#     model = MLP(n_hidden_nodes=100, n_classes=10)\n",
    "#     data = Data(batch_size=32)\n",
    "#     optimizer = SGD(model.parameters(), lr=0.005)\n",
    "#     trainer = ModelTrainer(model, data, optimizer, epochs=10, log_path=\"training.log\")\n",
    "\n",
    "#     # Training\n",
    "#     trainer.train()\n",
    "    \n",
    "#     # Evaluation\n",
    "#     trainer.evaluate()\n",
    "\n",
    "#     # Hyperparameter tuning\n",
    "#     param_grid = {\n",
    "#         'batch_size': [16, 32],\n",
    "#         'lr': [1e-3, 5e-3],\n",
    "#         'optimizer': ['SGD', 'Adam']\n",
    "#     }\n",
    "#     hyperparameter_tuning(model, param_grid, data, epochs=1)\n",
    "\n",
    "#     # Save model\n",
    "#     trainer.save_model(\"mlp_checkpoint.pth\")\n",
    "# # # \"\"\"\n",
    "\n",
    "# # Write the refactored code to a Python file\n",
    "# with open(\"/mnt/data/refactored_code.py\", \"w\", encoding=\"utf-8\") as file:\n",
    "#     file.write(refactored_code)\n",
    "\n",
    "# \"/mnt/data/refactored_code.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(n_hidden_nodes=100, n_classes=10, n_hidden_layers=2)\n",
    "data = Data(batch_size=32)\n",
    "optimizer = SGD(model.parameters(), lr=0.005)\n",
    "trainer = ModelTrainer(model, data, optimizer, epochs=1, log_path=\"training.log\")\n",
    "\n",
    "# Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
